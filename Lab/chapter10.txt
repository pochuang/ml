pcaimport numpy as np
from sklearn.decomposition import PCA
data = np.array([[-1,-1],[-2,-1],[-3,-2],[1,1],[2,1],[3,2]])
import matplotlib.pyplot as plt

x = data[:,0]
y = data[:,1]

>>> data
array([[-1, -1],
       [-2, -1],
       [-3, -2],
       [ 1,  1],
       [ 2,  1],
       [ 3,  2]])
>>> x
array([-1, -2, -3,  1,  2,  3])
>>> y
array([-1, -1, -2,  1,  1,  2])

>>> colors = ['b','g','r','c','m','y','k']
>>> labels = ['1','2','3','4','5','6']

>>> plt.scatter(x,y,color='blue')
<matplotlib.collections.PathCollection object at 0x11377ff10>

>>> for i in range(len(data)):
...     plt.scatter(data[i,0],data[i,1],c=colors[i],label=labels[i])

>>> plt.legend()

>>> plt.axvline(0.0, ls='dotted', color='k') 
>>> plt.axhline(0.0, ls='dotted', color='k')

>>> plt.show()

pca = PCA(n_components=2)

>>> pca.fit(data)
PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)

>>> print pca.explained_variance_ratio_
[ 0.99244289  0.00755711]

>>> pca.transform(data)
array([[ 1.38340578,  0.2935787 ],
       [ 2.22189802, -0.25133484],
       [ 3.6053038 ,  0.04224385],
       [-1.38340578, -0.2935787 ],
       [-2.22189802,  0.25133484],
       [-3.6053038 , -0.04224385]])

>>> data_x = pca.transform(data)

#whiten=‘True’將資料縮放到unit vector
>>> pca = PCA(n_components=2,whiten='True')
>>> pca.fit(data)
PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten='True')

>>> data_xw = pca.transform(data)
>>> data_xw
array([[ 0.53782682,  1.30795348],
       [ 0.86380754, -1.11974843],
       [ 1.40163435,  0.18820506],
       [-0.53782682, -1.30795348],
       [-0.86380754,  1.11974843],
       [-1.40163435, -0.18820506]])

>>> colors = ['b','g','r','c','m','y','k']
>>> labels = ['1','2','3','4','5','6']

>>> for i in range(len(data_x)):
...     plt.scatter(data_x[i,0],data_x[i,1],c=colors[i],label=labels[i])
...

>>> plt.legend()

>>> plt.axvline(0.0, ls='dotted', color='k') 
>>> plt.axhline(0.0, ls='dotted', color='k')
>>> plt.show()

>>> pca = PCA(n_components=1)
>>> pca.fit(data)
PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)
>>> print pca.explained_variance_ratio_
[ 0.99244289]
>>> print pca.explained_variance_
[ 6.61628593]
>>> print pca.components_
[[-0.83849224 -0.54491354]]
>>> data
array([[-1, -1],
       [-2, -1],
       [-3, -2],
       [ 1,  1],
       [ 2,  1],
       [ 3,  2]])

>>> pca.transform(data)
array([[ 1.38340578],
       [ 2.22189802],
       [ 3.6053038 ],
       [-1.38340578],
       [-2.22189802],
       [-3.6053038 ]])

>>> data_x = pca.transform(data)

>>> for i in range(len(data_x)):
...     plt.scatter(data_x[i],0,c=colors[i],label=labels[i])
...

>>> plt.axvline(0.0, ls='dotted', color='k') 
>>> plt.axhline(0.0, ls='dotted', color='k')
>>> plt.legend()
>>> plt.show()

#成績表
Frank,89,90,67,46,50
Wilson,57,70,80,85,90
Linda,80,90,35,40,50
Vivid,40,60,50,45,55
Jacky,78,85,45,55,60
Richard,55,65,80,75,85
Allen,90,85,88,92,95


import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
data = pd.read_csv('C:\pyml_scripts\chapter10_pca\scores.csv',names=['Name','Math','Science','Mandarin','English','Social'])
                      rows,cols
				 理科                文科  
>>> data    -------------  -------------------------
      Name  Math  Science  Mandarin  English  Social
0    Frank    89       90        67       46      50
1   Wilson    57       70        80       85      90
2    Linda    80       90        35       40      50
3    Vivid    40       60        50       45      55
4    Jacky    78       85        45       55      60
5  Richard    55       65        80       75      85
6    Allen    90       85        88       92      95
					  
>>> data_scores = data.iloc[:,1:]
>>> data_scores
   Math  Science  Mandarin  English  Social
0    89       90        67       46      50
1    57       70        80       85      90
2    80       90        35       40      50
3    40       60        50       45      55
4    78       85        45       55      60
5    55       65        80       75      85
6    90       85        88       92      95

 |
-|-----1
 |
 0
	
—-等同data_scores.mean(),為不同rows相同column的平均值
>>> data_scores.mean(axis=0)
Math        69.857143
Science     77.857143
Mandarin    63.571429
English     62.571429
Social      69.285714
dtype: float64

—相同row不同columns的平均值
>>> data_scores.mean(axis=1)
0    68.4
1    76.4
2    59.0
3    50.0
4    64.6
5    72.0
6    90.0
dtype: float64

--100-80    => A
--79-70     => B
--69-60     => C
--59-50     => D
--under 49  => E

>>> data_target=pd.DataFrame(
...     [
...         ["C"],["B"],["D"],["D"],["C"],["B"],["A"]
...     ]
... )
>>> data_target
   0
0  C
1  B
2  D
3  D
4  C
5  B
6  A

pca = PCA(n_components=2)

>>> pca.fit(data_scores)
PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)

>>> pca.n_components_
2
>>> pca.explained_variance_ratio_
array([ 0.6688013 ,  0.28791087])
>>> pca.components_
array([[-0.04318455, -0.11661043,  0.55136578,  0.60073709,  0.56537406],
       [-0.84543226, -0.51948621, -0.08791982, -0.08720053,  0.00667425]])
          Math          Science     Mandarin      English      Social     

>>> data_scores
   Math  Science  Mandarin  English  Social
0    89       90        67       46      50
1    57       70        80       85      90
2    80       90        35       40      50
3    40       60        50       45      55
4    78       85        45       55      60
5    55       65        80       75      85
6    90       85        88       92      95			 
>>> np.cumsum(pca.explained_variance_ratio_)
array([ 0.6688013 ,  0.95671218])

#進行維度減少
>>> data_x = pca.transform(data_scores)
>>> data_x
array([[-21.21097689, -21.47715546],
       [ 35.71460142,  11.68959258],
       [-42.0704435 , -10.53162768],
       [-22.74370588,  37.14882026],
       [-21.22256751,  -8.3637958 ],
       [ 27.54978153,  16.81652223],
       [ 43.98331082, -25.28235614]])
	   
—一次完成fit/transform
>>> PCA(n_components=2).fit_transform(data_scores)
array([[-21.21097689, -21.47715546],
       [ 35.71460142,  11.68959258],
       [-42.0704435 , -10.53162768],
       [-22.74370588,  37.14882026],
       [-21.22256751,  -8.3637958 ],
       [ 27.54978153,  16.81652223],
       [ 43.98331082, -25.28235614]])

>>> import matplotlib.pyplot as plt
>>> plt.ion()
>>> plt.clf()
>>> data['Name']
0      Frank
1     Wilson
2      Linda
3      Vivid
4      Jacky
5    Richard
6      Allen
Name: Name, dtype: object

--將pandas的series(一維)資料轉成Array
>>> name = data['Name'].ravel()
>>> name
array(['Frank', 'Wilson', 'Linda', 'Vivid', 'Jacky', 'Richard', 'Allen'], dtype=object)

>>> colors = ['b','g','r','c','m','y','k']

for i in range(len(name)):
    plt.scatter(data_x[i,0],data_x[i,1],c=colors[i],label=name[i])

plt.legend()
plt.show()




—已經過期,請改用from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>>> from sklearn.lda import LDA
/Users/Frank/anaconda/lib/python2.7/site-packages/sklearn/lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19
  "in 0.17 and will be removed in 0.19", DeprecationWarning)

>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>>> LDA = LinearDiscriminantAnalysis(n_components=2)
>>> LDA
LinearDiscriminantAnalysis(n_components=2, priors=None, shrinkage=None,
              solver='svd', store_covariance=False, tol=0.0001)

>>> y_target=data_target[0].ravel()
>>> lda_x=LDA.fit_transform(data_scores,y_target)
>>> lda_x
array([[ 2.75212596, -1.71432926],
       [-6.03901834,  0.79566534],
       [ 6.97496405, -0.48663249],
       [ 6.55378163,  1.50542224],
       [ 3.02094435, -0.30187461],
       [-3.64103126,  0.98721126],
       [-9.62176639, -0.78546249]])

>>> name = data['Name'].ravel()
>>> colors = ['b','g','r','c','m','y','k']
>>> for i in range(len(name)):
...     plt.scatter(lda_x[i,0],lda_x[i,1],c=colors[i],label=name[i])
...

>>> plt.legend()
>>> plt.show()
